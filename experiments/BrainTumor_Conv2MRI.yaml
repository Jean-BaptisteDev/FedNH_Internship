server_config:
  # learning strategy for serve: str; currently supported: {FeaAvg, FedDiscrete}
  strategy: None
  # number of communication rounds: int
  num_rounds: 100
  # number of total participating clients: int
  num_clients: 100
  # control number of participating clients per round: float [0, 1.0];
  participate_ratio: 0.1
  # randomly remove `drop_ratio` fraction of total participating clients at the aggregation time of each round: float [0.0, 1.0)
  drop_ratio: 0.0
  test_every: 1
  # split testset
  split_testset: False
  use_tqdm: False
  # dataset used for training: str; currently supported: {Mnist, FashionMnist, Cifar10}
  dataset: BrainTumorMRI
  # dataset partition strategy: str; {'iid-equal-size', 'iid-diff-size', 'noniid-label-quantity', 'noniid-label-distribution', 'shards'}
  partition: None # Choisissez ou ajustez une stratégie adaptée
  # float >0
  beta: None
  # int <= 10
  num_classes_per_client: None
  # int
  num_shards_per_client: None
  num_classes: 4
  learning_rate: 1.0
  lr_decay_per_round: 1.0
  # layers to be skipped in aggregation
  exclude: !!python/tuple []

  #################################################
  ##### algorithm specific settings go here ########
  #################################################
  # FedNH:
  FedNH_smoothing: 0.9
  FedNH_server_adv_prototype_agg: False

  # FedNHPlus:
  FedNHPlus_smoothing: 0.9
  FedNHPlus_server_adv_prototype_agg: False

  # CReFF
  CReFF_num_of_fl_feature: 100
  CReFF_match_epoch: 100
  CReFF_lr_net: 0.01
  CReFF_lr_feature: 0.1
  CReFF_crt_epoch: 300
  CReFF_dis_metric: 'ours'

client_config:
  # network used for each client
  model: Conv2MRI # Adapté au modèle pour les images MRI
  # network specific setting goes here

  # dataset setting
  # input size: size of a single input; 1 channel, 128*128 (ajustez selon la taille des images MRI)
  input_size: !!python/tuple [1, 128, 128]
  num_classes: 2 # Ajustez selon le nombre de classes du dataset
  # number of local epochs: int
  num_epochs: 5
  # number of samples per batch: int
  batch_size: 64
  # {Adam, SGD}
  optimizer: SGD
  # initial learning rate for each round
  learning_rate: 0.1
  lr_scheduler: stepwise # {diminishing}
  lr_decay_per_round: 0.99
  num_rounds: 100
  # other settings
  use_tqdm: False

  #################################################
  ##### algorithm specific settings go here ########
  #################################################

  # FedROD
  FedROD_hyper_clf: True
  FedROD_phead_separate: False # this is very expensive

  # FedNH
  FedNH_return_embedding: False
  FedNH_head_init: orthogonal
  FedNH_client_adv_prototype_agg: False
  FedNH_fix_scaling: False

  # FedNHPlus
  FedNHPlus_return_embedding: False
  FedNHPlus_head_init: orthogonal
  FedNHPlus_client_adv_prototype_agg: False
  FedNHPlus_fix_scaling: False

  # FedProto
  FedProto_lambda: 0.1

  # FedRep
  FedRep_head_epochs: 10

  # FedBABU
  FedBABU_finetune_epoch: 5

  # Ditto
  Ditto_lambda: 0.75 # penalty parameter for Ditto follows the setting of FedRep

  # CReFF
  CReFF_batch_real: 64
